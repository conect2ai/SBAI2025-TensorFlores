{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e610dd",
   "metadata": {},
   "source": [
    "# Quantization-Aware Training (QAT)\n",
    "\n",
    "This Jupyter Notebook provides a comprehensive walkthrough of **Quantization-Aware Training (QAT)**. Unlike Post-Training Quantization (PTQ), where a fully-trained model is converted to a lower precision format, QAT simulates the effects of this lower precision *during* the training process itself. This allows the model's weights and biases to adapt to the constraints of quantization, which often results in higher accuracy for the final, compressed model.\n",
    "\n",
    "The key steps in this notebook are:\n",
    "1.  **Data Loading**: We will load the training and testing datasets from Weights & Biases (W&B) artifacts.\n",
    "2.  **Model Configuration**: We will define the architecture and hyperparameters for a Multilayer Perceptron model.\n",
    "3.  **Quantization-Aware Training**: We will train the model from scratch while applying quantization-aware techniques.\n",
    "4.  **Hyperparameter Sweep**: We will perform an automated search across various configurations to identify the optimal model, tracking every experiment with W&B.\n",
    "5.  **Artifact Logging**: The resulting models will be saved as C++ and JSON files and logged as versioned artifacts to W&B for deployment and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931fdd7b",
   "metadata": {},
   "source": [
    "## 1. Library Imports\n",
    "\n",
    "We begin by importing the necessary Python libraries. These packages provide the core functionalities for data handling, model building, and experiment tracking.\n",
    "\n",
    "* `wandb`: The primary tool for logging experiments, managing configurations, and versioning artifacts.\n",
    "* `os`: Used for interacting with the file system, particularly for creating file paths.\n",
    "* `pandas` & `numpy`: Essential for data manipulation and high-performance numerical computation.\n",
    "* `sklearn.model_selection`: Provides tools for splitting datasets.\n",
    "* `tensorflores.models.multilayer_perceptron`: Our custom implementation of a Multilayer Perceptron that supports QAT.\n",
    "* `tensorflores.utils.clustering`: Contains the clustering algorithms used for the quantization process.\n",
    "* `itertools`: A Python module for creating iterators for efficient looping, used here to generate hyperparameter combinations.\n",
    "* `time`: To measure the duration of the training processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15675b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tensorflores.models.multilayer_perceptron import MultilayerPerceptron\n",
    "from tensorflores.utils.clustering import ClusteringMethods\n",
    "\n",
    "# To run this notebook, you need a Wandb account and an API key.\n",
    "# You can create a file named my_key.py with the line: WANDB_KEY = 'your_api_key_here'\n",
    "# and then uncomment the line below.\n",
    "from my_key import WANDB_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996e314e",
   "metadata": {},
   "source": [
    "## 2. Weights & Biases Initialization\n",
    "\n",
    "To ensure every part of our training pipeline is reproducible and tracked, we connect to the Weights & Biases platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f75a5",
   "metadata": {},
   "source": [
    "### 2.1. Authentication\n",
    "\n",
    "First, we log in using a W&B API key.\n",
    "\n",
    "**Security Best Practice**: It is highly recommended to manage API keys using environment variables (`WANDB_API_KEY`) or the W&B CLI (`wandb login`) rather than hardcoding them in source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9003c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key = WANDB_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046dd2a0",
   "metadata": {},
   "source": [
    "### 2.2. Initial Run for Data Loading\n",
    "\n",
    "We start a preliminary run with the `job_type` set to `data-loading`. This helps organize our project by separating the data preparation stage from model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c514cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run =  wandb.init(project = \"SBAI 2025\", job_type = \"data-loading\", save_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d938afde",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preparation\n",
    "\n",
    "The model's performance is dependent on the data it's trained on. We will download our versioned training and test datasets directly from W&B artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878cef1",
   "metadata": {},
   "source": [
    "### 3.1. Download Datasets\n",
    "\n",
    "We use `run.use_artifact` to specify the desired dataset versions and `artifact.download()` to retrieve them locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428985f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training dataset\n",
    "artifact = run.use_artifact(artifact_or_name = \"thommasflores-ufrn/SBAI 2025/train_dataset:latest\")\n",
    "path = artifact.download()\n",
    "\n",
    "# List files to identify the CSV\n",
    "print(\"Files in the downloaded directory:\", os.listdir(path))\n",
    "csv_file_path_train = os.path.join(path, os.listdir(path)[0])\n",
    "df_train = pd.read_csv(csv_file_path_train)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the test dataset\n",
    "artifact = run.use_artifact(artifact_or_name = \"thommasflores-ufrn/SBAI 2025/test_dataset:latest\")\n",
    "path = artifact.download()\n",
    "csv_file_path_test = os.path.join(path, os.listdir(path)[0])\n",
    "df_test = pd.read_csv(csv_file_path_test)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4961c0",
   "metadata": {},
   "source": [
    "### 3.2. Prepare Data for Training\n",
    "\n",
    "We process the data by separating it into features (inputs) and the target variable, and then converting the pandas DataFrames into NumPy arrays, which is the required format for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ab995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and target columns\n",
    "target = ['CO2 (g/s) [estimated maf]']\n",
    "input = ['intake_pressure','intake_temperature','rpm', 'speed']\n",
    "\n",
    "# Separate features and targets\n",
    "X_train = df_train[input]\n",
    "y_train = df_train[target]\n",
    "X_test = df_test[input]\n",
    "y_test = df_test[target]\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_value_train = X_train.values\n",
    "y_value_train = y_train.values\n",
    "X_value_test = X_test.values\n",
    "y_value_test = y_test.values\n",
    "\n",
    "print('Train input shape: ', X_value_train.shape)\n",
    "print('Train output shape: ', y_value_train.shape)\n",
    "print('Test input shape: ', X_value_test.shape)\n",
    "print('Test output shape: ', y_value_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595baca",
   "metadata": {},
   "source": [
    "## 4. Single Quantization-Aware Training (QAT) Run\n",
    "\n",
    "Before performing a large-scale hyperparameter sweep, it is instructive to walk through a single, complete QAT cycle. This helps in understanding the components involved in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b261d",
   "metadata": {},
   "source": [
    "### 4.1. Define Quantization and Model Parameters\n",
    "\n",
    "In QAT, the quantization method is a key part of the training configuration. We use clustering algorithms to quantize the weights and biases.\n",
    "\n",
    "* **Clustering Method**: We choose from several options to group the model's parameters. Here, we select `autocloud`, a density-based algorithm. Other choices include `meanshift`, `affinity_propagation`, and `dbstream`.\n",
    "* **Distance Metric**: This metric is used by the clustering algorithm to measure the similarity between parameter values. We will use the `euclidean` distance. Other common choices include `manhattan` and `cosine`.\n",
    "* **Model Configuration**: All hyperparameters are defined in a `config` dictionary. This includes network architecture (`hidden_layer_sizes`, `activation_functions`), training parameters (`learning_rate`, `epochs`), and QAT settings (`training_with_quantization`, `epochs_quantization`). Logging this dictionary to W&B ensures complete reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Clustering Method\n",
    "Clustering_method = ClusteringMethods()\n",
    "\n",
    "bias_clustering_method = Clustering_method.autocloud_biases(threshold_biases = 1.4148)\n",
    "weight_clustering_method = Clustering_method.autocloud_weight(threshold_weights = 1.4148)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full configuration for a single run\n",
    "config = {\n",
    "    'input_size': 4,\n",
    "    'output_size': 1,\n",
    "    'hidden_layer_sizes': [16, 8],\n",
    "    'activation_functions': ['relu', 'relu', 'linear'],\n",
    "    'weight_bias_init': 'RandomNormal',\n",
    "    'training_with_quantization': True,\n",
    "    'epochs': 100,\n",
    "    'epochs_quantization': 100,\n",
    "    'learning_rate': 0.001,\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'optimizer': 'adamax',\n",
    "    'batch_size': 36,\n",
    "    'validation_split': 0.2,\n",
    "    'distance_metric': \"euclidean\",\n",
    "}\n",
    "\n",
    "# Initialize the W&B run with the specified config\n",
    "wandb.init(project=\"SBAI 2025\", job_type = \"training-model-QAT\", config=config, save_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b33c34",
   "metadata": {},
   "source": [
    "### 4.2. Model Instantiation and Training\n",
    "\n",
    "We create an instance of our `MultilayerPerceptron` and call the `.train()` method. The `training_with_quantization=True` flag enables the QAT process. The model first trains normally for `epochs` and then fine-tunes the quantized parameters for `epochs_quantization`, allowing it to recover accuracy lost during the initial quantization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d12cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "nn = MultilayerPerceptron(input_size=config['input_size'],\n",
    "                          output_size=config['output_size'],\n",
    "                          hidden_layer_sizes=config['hidden_layer_sizes'],\n",
    "                          activation_functions=config['activation_functions'],\n",
    "                          weight_bias_init=config['weight_bias_init'],\n",
    "                          training_with_quantization=config['training_with_quantization'])\n",
    "\n",
    "# Train the model with QAT\n",
    "nn.train(X=X_value_train, \n",
    "         y=y_value_train,\n",
    "         epochs=config['epochs'], \n",
    "         epochs_quantization = config['epochs_quantization'],\n",
    "         learning_rate=config['learning_rate'],\n",
    "         loss_function=config['loss_function'], \n",
    "         optimizer=config['optimizer'], \n",
    "         batch_size=config['batch_size'], \n",
    "         validation_split=config['validation_split'],\n",
    "         distance_metric = config['distance_metric'],\n",
    "         bias_clustering_method = bias_clustering_method,\n",
    "         weight_clustering_method = weight_clustering_method,         \n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7437c8c0",
   "metadata": {},
   "source": [
    "### 4.3. Evaluation and Logging\n",
    "\n",
    "After training, we evaluate the model's performance on the unseen test set by calculating the Mean Squared Error (MSE) and other error metrics. These results are logged to W&B to track the performance of this specific configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and calculate the error\n",
    "pred = nn.predict(X_value_test)\n",
    "error = pred - y_value_test\n",
    "mse = np.mean(np.square(error))\n",
    "\n",
    "# Log metrics to wandb and finish the run\n",
    "wandb.log({\n",
    "    'mse_test': mse,\n",
    "    'mean_error': np.mean(error),\n",
    "    'mean_absolute_error': np.mean(np.abs(error)),\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec32fdf",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Sweep for Optimal QAT Model\n",
    "\n",
    "While a single run is useful for understanding the process, finding the best model requires exploring multiple hyperparameter combinations. A hyperparameter sweep (or grid search) automates this exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d2e0b",
   "metadata": {},
   "source": [
    "### 5.1. Define the Hyperparameter Search Space\n",
    "\n",
    "We create lists of possible values for each hyperparameter we want to test. This includes network architectures, activation functions, learning rates, and quantization settings. The `itertools.product` function is then used to generate a list of all unique combinations from these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sets of hyperparameters to explore\n",
    "activation_sets = [\n",
    "    ['relu', 'relu', 'linear'],\n",
    "    ['tanh', 'relu', 'linear'],\n",
    "    ['sigmoid', 'sigmoid', 'linear']\n",
    "]\n",
    "\n",
    "hidden_layer_sets = [\n",
    "    [16, 8],\n",
    "    [32, 16],\n",
    "    [64, 32]\n",
    "]\n",
    "\n",
    "learning_rates = [0.01, 0.001]\n",
    "\n",
    "bias_clustering_methods = [ \n",
    "    Clustering_method.autocloud_biases(threshold_biases=1.4148),\n",
    "    Clustering_method.meanshift_biases(bandwidth_biases=0.005)\n",
    "]\n",
    "\n",
    "weight_clustering_methods = [ \n",
    "    Clustering_method.autocloud_weight(threshold_weights=1.4148),\n",
    "    Clustering_method.meanshift_weight(bandwidth_weights=0.005)\n",
    "]\n",
    "\n",
    "distance_metrics = [\"euclidean\", \"minkowski\"]\n",
    "epochs_quantization = [100, 60]\n",
    "\n",
    "# Create all possible combinations\n",
    "combinations = list(itertools.product(\n",
    "    activation_sets,\n",
    "    hidden_layer_sets,\n",
    "    learning_rates,\n",
    "    bias_clustering_methods,\n",
    "    weight_clustering_methods,\n",
    "    distance_metrics,\n",
    "    epochs_quantization\n",
    "))\n",
    "\n",
    "print(f\"Total combinations to be tested: {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130bcd4",
   "metadata": {},
   "source": [
    "### 5.2. Executing the Sweep\n",
    "\n",
    "We now loop through every combination generated. For each one, the script will:\n",
    "1.  Define the `config` dictionary for the specific trial.\n",
    "2.  Initialize a new, separate W&B run to track the trial.\n",
    "3.  Instantiate and train the model with the given configuration.\n",
    "4.  Measure the training time.\n",
    "5.  Evaluate the final model's performance on both train and test sets.\n",
    "6.  Save the quantized model as deployable `tensorflores_QAT.h` (C++) and `tensorflores_QAT.json` files.\n",
    "7.  Log these files as versioned artifacts to the W&B run.\n",
    "8.  Log all performance metrics (MSE, MAE, training time) to W&B.\n",
    "9.  Finalize the run.\n",
    "\n",
    "This systematic approach allows us to use the W&B dashboard to easily compare all configurations and identify the one that provides the best balance of accuracy and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686885c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100, Loss: 1.6206464560251592, Val Loss: 3.961390921629187, Bias Clusters: 67, Weight Clusters: 51\n"
     ]
    }
   ],
   "source": [
    "for act_funcs, hidden_layers, lr, bias_method, weight_method, dist_metric, quant_epochs in combinations:\n",
    "    # Define the configuration for the current experiment\n",
    "    config = {\n",
    "        'input_size': 4,\n",
    "        'output_size': 1,\n",
    "        'hidden_layer_sizes': hidden_layers,\n",
    "        'activation_functions': act_funcs,\n",
    "        'weight_bias_init': 'RandomNormal',\n",
    "        'training_with_quantization': True,\n",
    "        'epochs': 100,\n",
    "        'learning_rate': lr,\n",
    "        'loss_function': 'mean_squared_error',\n",
    "        'optimizer': 'adamax',\n",
    "        'batch_size': 36,\n",
    "        'validation_split': 0.2,\n",
    "        'bias_clustering_method': bias_method,\n",
    "        'weight_clustering_method': weight_method,\n",
    "        'distance_metric': dist_metric,\n",
    "        'epochs_quantization': quant_epochs\n",
    "    }\n",
    "\n",
    "    # Initialize a new W&B run for this combination\n",
    "    wandb.init(project=\"SBAI 2025\", job_type=\"training-model-QAT-sweep\", config=config, save_code=True)\n",
    "\n",
    "    # Instantiate and train the model\n",
    "    nn = MultilayerPerceptron(\n",
    "        input_size=config['input_size'],\n",
    "        output_size=config['output_size'],\n",
    "        hidden_layer_sizes=config['hidden_layer_sizes'],\n",
    "        activation_functions=config['activation_functions'],\n",
    "        weight_bias_init=config['weight_bias_init'],\n",
    "        training_with_quantization=config['training_with_quantization'],\n",
    "\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    nn.train(\n",
    "        X=X_value_train,\n",
    "        y=y_value_train,\n",
    "        epochs=config['epochs'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        loss_function=config['loss_function'],\n",
    "        optimizer=config['optimizer'],\n",
    "        batch_size=config['batch_size'],\n",
    "        validation_split=config['validation_split'],\n",
    "        bias_clustering_method=config['bias_clustering_method'],\n",
    "        weight_clustering_method=config['weight_clustering_method'],\n",
    "        distance_metric=config['distance_metric'],\n",
    "        epochs_quantization=config['epochs_quantization']\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # Evaluate the trained model\n",
    "    def evaluate(model, X, y):\n",
    "        pred = model.predict(X)\n",
    "        error = pred - y\n",
    "        return np.mean(np.square(error)), np.mean(np.abs(error))\n",
    "\n",
    "    mse_train, mae_train = evaluate(nn, X_value_train, y_value_train)\n",
    "    mse_test, mae_test = evaluate(nn, X_value_test, y_value_test)\n",
    "\n",
    "    # Save the model in C++ and JSON formats\n",
    "    nn.save_model_as_cpp('./cpp_models/tensorflores_QAT')\n",
    "    nn.save_model_as_json('./json_models/tensorflores_QAT')\n",
    "\n",
    "    # Upload the saved models as W&B artifacts\n",
    "    cpp_artifact = wandb.Artifact(\"cpp_QAT\", type=\"model_QAT\")\n",
    "    cpp_artifact.add_file('./cpp_models/tensorflores_QAT.h')\n",
    "    wandb.log_artifact(cpp_artifact)\n",
    "\n",
    "    json_artifact = wandb.Artifact(\"json_QAT\", type=\"model_QAT\")\n",
    "    json_artifact.add_file('./json_models/tensorflores_QAT.json')\n",
    "    wandb.log_artifact(json_artifact)\n",
    "\n",
    "    # Log performance metrics and training time\n",
    "    wandb.log({\n",
    "        'train_time': train_time,\n",
    "        'mse_train': mse_train,\n",
    "        'mae_train': mae_train,\n",
    "        'mse_test': mse_test,\n",
    "        'mae_test': mae_test\n",
    "    })\n",
    "\n",
    "    # Finish the W&B run for this combination\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
