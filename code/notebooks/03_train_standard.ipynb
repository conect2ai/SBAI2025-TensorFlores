{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Training a CO2 Emissions Prediction Model**\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "This notebook marks the third stage of our MLOps pipeline: model training. Leveraging the preprocessed and segregated data from the previous step, our goal is to train a Multilayer Perceptron (MLP) model to predict CO2 emissions.\n",
        "\n",
        "The main activities in this script are:\n",
        "* Loading the versioned training and testing datasets from Weights & Biases (Wandb).\n",
        "* Preparing the data for the model by separating features and the target variable.\n",
        "* Conducting a systematic hyperparameter sweep to find the best model configuration.\n",
        "* Training the model with each configuration.\n",
        "* Evaluating the model's performance on both training and testing data.\n",
        "* Logging all hyperparameters, performance metrics, and saving the final model files as artifacts in Wandb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **2. Library Imports**\n",
        "\n",
        "We start by importing the necessary libraries for our tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for a cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# tensorflores is a custom library for the Multilayer Perceptron.\n",
        "# Make sure it is correctly installed and accessible in your environment.\n",
        "from tensorflores.models.multilayer_perceptron import MultilayerPerceptron\n",
        "\n",
        "\n",
        "# To run this notebook, you need a Wandb account and an API key.\n",
        "# You can create a file named my_key.py with the line: WANDB_KEY = 'your_api_key_here'\n",
        "# and then uncomment the line below.\n",
        "from my_key import WANDB_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **3. Loading Versioned Datasets**\n",
        "\n",
        "To ensure reproducibility, we do not use local files directly. Instead, we pull the `train_dataset` and `test_dataset` directly from our Wandb project. This guarantees that we are training our model on the exact same data that was processed in the previous stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a temporary Wandb run to download the artifacts.\n",
        "# This run will be closed once the data is loaded.\n",
        "with wandb.init(project=\"SBAI 2025\", job_type=\"data-loading\") as run:\n",
        "    \n",
        "    # --- Load Training Data ---\n",
        "    train_artifact = run.use_artifact(\"train_dataset:latest\")\n",
        "    train_path = train_artifact.download()\n",
        "    train_csv_path = os.path.join(train_path, os.listdir(train_path)[0])\n",
        "    df_train = pd.read_csv(train_csv_path)\n",
        "    print(\"Training dataset loaded successfully.\")\n",
        "    display(df_train.head())\n",
        "\n",
        "    # --- Load Testing Data ---\n",
        "    test_artifact = run.use_artifact(\"test_dataset:latest\")\n",
        "    test_path = test_artifact.download()\n",
        "    test_csv_path = os.path.join(test_path, os.listdir(test_path)[0])\n",
        "    df_test = pd.read_csv(test_csv_path)\n",
        "    print(\"Testing dataset loaded successfully.\")\n",
        "    display(df_test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **4. Data Preparation**\n",
        "\n",
        "With the datasets loaded, we need to separate them into input features (X) and the target variable (y). We also convert the pandas DataFrames into NumPy arrays, which is the expected input format for our `MultilayerPerceptron` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the target and input feature columns\n",
        "target_column = ['CO2 (g/s) [estimated maf]']\n",
        "feature_columns = ['intake_pressure', 'intake_temperature', 'rpm', 'speed']\n",
        "\n",
        "# --- Prepare Training Data ---\n",
        "X_train = df_train[feature_columns].values\n",
        "y_train = df_train[target_column].values\n",
        "\n",
        "# --- Prepare Testing Data ---\n",
        "X_test = df_test[feature_columns].values\n",
        "y_test = df_test[target_column].values\n",
        "\n",
        "print(f\"Data prepared for training and testing.\")\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **5. Hyperparameter Sweep (Model Training Loop)**\n",
        "\n",
        "Instead of training a single model, we will perform a **hyperparameter sweep**. This involves systematically training multiple models with different configurations to find the one that performs best. We will experiment with different activation functions, hidden layer architectures, and learning rates.\n",
        "\n",
        "Each combination of these hyperparameters will constitute a single, tracked experiment in Wandb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(\"cpp_models\", exist_ok=True)\n",
        "os.makedirs(\"json_models\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the hyperparameter grid for our sweep\n",
        "activation_sets = [\n",
        "    ['relu', 'relu', 'linear'],\n",
        "    ['tanh', 'relu', 'linear'],\n",
        "    ['sigmoid', 'sigmoid', 'linear']\n",
        "]\n",
        "hidden_layer_sets = [\n",
        "    [16, 8],\n",
        "    [32, 16],\n",
        "    [64, 32]\n",
        "]\n",
        "learning_rates = [0.01, 0.001]\n",
        "\n",
        "print(\"Starting hyperparameter sweep...\")\n",
        "\n",
        "# Loop through each combination of hyperparameters\n",
        "for act_funcs in activation_sets:\n",
        "    for hidden_layers in hidden_layer_sets:\n",
        "        for lr in learning_rates:\n",
        "\n",
        "            # 1. Define the configuration for the current run\n",
        "            config = {\n",
        "                'input_size': X_train.shape[1],\n",
        "                'output_size': y_train.shape[1],\n",
        "                'hidden_layer_sizes': hidden_layers,\n",
        "                'activation_functions': act_funcs,\n",
        "                'weight_bias_init': 'RandomNormal',\n",
        "                'training_with_quantization': False,\n",
        "                'epochs': 100,  # A fixed number of epochs for this sweep\n",
        "                'learning_rate': lr,\n",
        "                'loss_function': 'mean_squared_error',\n",
        "                'optimizer': 'adamax',\n",
        "                'batch_size': 36,\n",
        "                'validation_split': 0.2\n",
        "            }\n",
        "\n",
        "            # 2. Initialize a new Wandb run for this specific configuration\n",
        "            with wandb.init(project=\"SBAI 2025\", job_type=\"training\", config=config, save_code=True) as run:\n",
        "                \n",
        "                # 3. Instantiate and train the model\n",
        "                print(f\"Training with config: layers={hidden_layers}, activations={act_funcs}, lr={lr}\")\n",
        "                nn = MultilayerPerceptron(\n",
        "                    input_size=config['input_size'],\n",
        "                    output_size=config['output_size'],\n",
        "                    hidden_layer_sizes=config['hidden_layer_sizes'],\n",
        "                    activation_functions=config['activation_functions'],\n",
        "                    weight_bias_init=config['weight_bias_init'],\n",
        "                    training_with_quantization=config['training_with_quantization']\n",
        "                )\n",
        "\n",
        "                start_time = time.time()\n",
        "                nn.train(\n",
        "                    X=X_train,\n",
        "                    y=y_train,\n",
        "                    epochs=config['epochs'],\n",
        "                    learning_rate=config['learning_rate'],\n",
        "                    loss_function=config['loss_function'],\n",
        "                    optimizer=config['optimizer'],\n",
        "                    batch_size=config['batch_size'],\n",
        "                    validation_split=config['validation_split']\n",
        "                )\n",
        "                train_time = time.time() - start_time\n",
        "                print(f\"Training finished in {train_time:.2f} seconds.\")\n",
        "\n",
        "                # 4. Evaluate the model and calculate metrics\n",
        "                y_pred_test = nn.predict(X_test)\n",
        "                mse_test = np.mean(np.square(y_pred_test - y_test))\n",
        "                mae_test = np.mean(np.abs(y_pred_test - y_test))\n",
        "\n",
        "                y_pred_train = nn.predict(X_train)\n",
        "                mse_train = np.mean(np.square(y_pred_train - y_train))\n",
        "                mae_train = np.mean(np.abs(y_pred_train - y_train))\n",
        "\n",
        "                # 5. Save the model in different formats\n",
        "                model_base_name = f'model_{run.id}'\n",
        "                nn.save_model_as_cpp('./cpp_models/' + model_base_name)\n",
        "                nn.save_model_as_json('./json_models/' + model_base_name)\n",
        "\n",
        "                # 6. Log metrics and model artifacts to Wandb\n",
        "                metrics_to_log = {\n",
        "                    'train_time': train_time,\n",
        "                    'mse_train': mse_train,\n",
        "                    'mae_train': mae_train,\n",
        "                    'mse_test': mse_test,\n",
        "                    'mae_test': mae_test\n",
        "                }\n",
        "                wandb.log(metrics_to_log)\n",
        "                \n",
        "                cpp_artifact = wandb.Artifact(name=f\"model-cpp-{run.id}\", type=\"model\")\n",
        "                cpp_artifact.add_file('./cpp_models/' + f'{model_base_name}.h')\n",
        "                wandb.log_artifact(cpp_artifact)\n",
        "\n",
        "                json_artifact = wandb.Artifact(name=f\"model-json-{run.id}\", type=\"model\")\n",
        "                json_artifact.add_file('./json_models/' +  f'{model_base_name}.json')\n",
        "                wandb.log_artifact(json_artifact)\n",
        "                \n",
        "                print(f\"Run {run.id} finished and logged.\")\n",
        "\n",
        "print(\"Hyperparameter sweep completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **6. Conclusion**\n",
        "\n",
        "This notebook has automated the process of training and evaluating multiple models. By performing a hyperparameter sweep, we can systematically explore different model architectures and training parameters. Each run is logged in Weights & Biases, providing a comprehensive overview of performance metrics and linking them to the specific configurations that produced them.\n",
        "\n",
        "After the sweep is complete, you can analyze the results in the Wandb dashboard to select the best-performing model for deployment or further analysis. The saved model artifacts (`.h` and `.json` files) are versioned and ready to be used in the next stage of the pipeline."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
